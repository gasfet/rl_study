{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** start random search *****\n",
      "*config: {'layer_num_actor': 1, 'node_num_actor': 115, 'epochs_actor': 4, 'layer_num_critic': 2, 'node_num_critic': 37, 'epochs_critic': 5, 'learning_rate_actor': 0.0007144842671363032, 'learning_rate_critic': 0.0007104724029696896, 'discount_rate': 0.9688835618788458, 'smooth_rate': 0.9176809055856051, 'penalty': -156, 'mini_batch_step_size': 71, 'loss_clipping': 0.13103136042461513}\n",
      "*result: 0 181.65 54.315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*config: {'layer_num_actor': 1, 'node_num_actor': 105, 'epochs_actor': 4, 'layer_num_critic': 1, 'node_num_critic': 84, 'epochs_critic': 4, 'learning_rate_actor': 0.00011337755140372159, 'learning_rate_critic': 0.000732430979644101, 'discount_rate': 0.927844296059655, 'smooth_rate': 0.9590013979191802, 'penalty': -264, 'mini_batch_step_size': 57, 'loss_clipping': 0.2397335589501082}\n",
      "*result: 1 32.25 28.085\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*config: {'layer_num_actor': 2, 'node_num_actor': 74, 'epochs_actor': 5, 'layer_num_critic': 1, 'node_num_critic': 75, 'epochs_critic': 6, 'learning_rate_actor': 0.000231782398737788, 'learning_rate_critic': 0.0009734276052834432, 'discount_rate': 0.9207354033530273, 'smooth_rate': 0.9314177791283295, 'penalty': -496, 'mini_batch_step_size': 4, 'loss_clipping': 0.2795545118780086}\n",
      "*result: 2 279.1 344.56\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*config: {'layer_num_actor': 1, 'node_num_actor': 51, 'epochs_actor': 4, 'layer_num_critic': 1, 'node_num_critic': 117, 'epochs_critic': 4, 'learning_rate_actor': 0.0003897398733672883, 'learning_rate_critic': 0.00019262764004991317, 'discount_rate': 0.9698649016319952, 'smooth_rate': 0.9509109596023608, 'penalty': -328, 'mini_batch_step_size': 53, 'loss_clipping': 0.22596953806637307}\n",
      "*result: 3 53.9 24.805\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*config: {'layer_num_actor': 1, 'node_num_actor': 34, 'epochs_actor': 4, 'layer_num_critic': 2, 'node_num_critic': 25, 'epochs_critic': 4, 'learning_rate_actor': 0.0007423292868442369, 'learning_rate_critic': 0.0006220065633523508, 'discount_rate': 0.9353355597855175, 'smooth_rate': 0.9300407200297724, 'penalty': -443, 'mini_batch_step_size': 45, 'loss_clipping': 0.25905618079602055}\n",
      "*result: 4 338.7 141.545\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*config: {'layer_num_actor': 1, 'node_num_actor': 94, 'epochs_actor': 5, 'layer_num_critic': 2, 'node_num_critic': 126, 'epochs_critic': 4, 'learning_rate_actor': 0.00048701931058286226, 'learning_rate_critic': 0.0003149320162002832, 'discount_rate': 0.9739738608317619, 'smooth_rate': 0.9581902295599934, 'penalty': -449, 'mini_batch_step_size': 11, 'loss_clipping': 0.26355532210150656}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, config_data):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.layer_num_actor = config_data['layer_num_actor']\n",
    "        self.node_num_actor = config_data['node_num_actor']\n",
    "        self.epochs_actor = config_data['epochs_actor']\n",
    "        self.layer_num_critic = config_data['layer_num_critic']\n",
    "        self.node_num_critic = config_data['node_num_critic']\n",
    "        self.epochs_critic = config_data['epochs_critic']\n",
    "        \n",
    "        self.learning_rate_actor = config_data['learning_rate_actor']\n",
    "        self.learning_rate_critic = config_data['learning_rate_critic']\n",
    "        self.discount_rate = config_data['discount_rate']\n",
    "        self.smooth_rate = config_data['smooth_rate']\n",
    "        self.penalty = config_data['penalty']\n",
    "        self.mini_batch_step_size = config_data['mini_batch_step_size']\n",
    "        self.loss_clipping = config_data['loss_clipping']\n",
    "\n",
    "        self.episode_num = 200\n",
    "        self.moving_avg_size = 20\n",
    "        \n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    " \n",
    "        self.states, self.states_next, self.action_matrixs, self.dones, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE = np.zeros((1,1,self.action_size)), np.zeros((1,1,self.value_size))\n",
    "    \n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages, loss_clipping = in_datas[0], in_datas[1], in_datas[2], in_datas[3]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)    \n",
    "                r = new_policy/(old_policy)\n",
    "                \n",
    "                LOSS_CLIPPING = K.mean(loss_clipping)\n",
    "                \n",
    "                loss = -K.minimum(r*advantages, K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)*advantages)\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(1,self.value_size), name='input_advantages')\n",
    "        input_loss_clipping = Input(shape=(1,self.value_size), name='input_loss_clipping')        \n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_actor+1):            \n",
    "            x = Dense(self.node_num_actor, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages, input_loss_clipping],\n",
    "                             outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_actor))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_critic+1):\n",
    "            x = Dense(self.node_num_critic, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_critic),\n",
    "#                       loss='mean_squared_error'\n",
    "                      loss = \"binary_crossentropy\"\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "            \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states, self.states_next, self.action_matrixs, self.done, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        advantage = np.zeros(self.value_size)\n",
    "        target = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(self.normalize(state),[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "            \n",
    "            state_next_t = np.reshape(self.normalize(state_next),[1, 1, self.state_size])\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "        \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.states_next.append(np.reshape(state_next_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.dones.append(np.reshape(0 if done else 1, [1,self.value_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(np.reshape(reward, [1,self.value_size]))\n",
    "            \n",
    "            if(count % self.mini_batch_step_size == 0):\n",
    "                self.train_mini_batch()\n",
    "                self.clear_memory()\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "            \n",
    "        return count, reward_tot\n",
    "    \n",
    "    def make_gae(self, values, values_next, rewards, dones):\n",
    "        delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "        advantages = np.zeros(np.array(values).shape)\n",
    "        targets = np.zeros(np.array(values).shape)\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            delta_adv = rewards[t] + self.discount_rate * values_next[t] * dones[t] - values[t]\n",
    "            delta_tar = rewards[t] + self.discount_rate * values_next[t] * dones[t]\n",
    "            adv = delta_adv + self.smooth_rate *  self.discount_rate * dones[t] * adv\n",
    "            target = delta_tar + self.smooth_rate * self.discount_rate * dones[t] * target\n",
    "            advantages[t] = adv\n",
    "            targets[t] = target\n",
    "        return advantages, targets\n",
    "\n",
    "    def normalize(self, x):\n",
    "#         current_min = np.min(x)\n",
    "#         current_max = np.max(x)\n",
    "#         x_normed = (x - current_min) / (current_max - current_min)\n",
    "#         current_mean = np.mean(x)\n",
    "#         current_std = np.std(x)\n",
    "#         x_normed = (x - current_mean) / current_std\n",
    "#         return x_normed\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: \n",
    "            return x\n",
    "        return x / norm\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        \n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        states_t = np.array(self.states)\n",
    "        states_next_t = np.array(self.states_next)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        loss_clipping = [self.loss_clipping for j in range(len(self.states))]\n",
    "        loss_clipping_t = np.reshape(loss_clipping, [len(self.states),1,1])\n",
    "        \n",
    "        values = self.model_critic.predict(states_t)\n",
    "        values_next = self.model_critic.predict(states_next_t)\n",
    "        \n",
    "        advantages, targets = self.make_gae(values, values_next, self.rewards, self.dones)\n",
    "        advantages_t = np.array(advantages)\n",
    "        targets_t = np.array(targets)\n",
    "        \n",
    "        self.model_actor.fit([states_t, action_matrixs_t, advantages_t, loss_clipping_t], [action_probs_t], \n",
    "                             epochs=self.epochs_actor, verbose=0)\n",
    "        self.model_critic.fit(states_t, targets_t, \n",
    "                              epochs=self.epochs_critic, verbose=0)       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def random_select():\n",
    "        config_data = {\n",
    "            'layer_num_actor':rand.randint(1,2),\n",
    "            'node_num_actor':rand.randint(12,128),\n",
    "            'epochs_actor':rand.randint(3,6),\n",
    "            'layer_num_critic':rand.randint(1,2),\n",
    "            'node_num_critic':rand.randint(12,128),\n",
    "            'epochs_critic':rand.randint(3,6),\n",
    "            \n",
    "            'learning_rate_actor' :rand.uniform(0.0001,0.001),\n",
    "            'learning_rate_critic':rand.uniform(0.0001,0.001),\n",
    "            'discount_rate'       :rand.uniform(0.9,0.99),\n",
    "            'smooth_rate'       :rand.uniform(0.9,0.99),\n",
    "            'penalty'             :rand.randint(-500,-10),\n",
    "            'mini_batch_step_size':rand.randint(4,80),\n",
    "            'loss_clipping'       :rand.uniform(0.1,0.3)\n",
    "        }\n",
    "        return config_data\n",
    "\n",
    "    results = []\n",
    "    print(\"***** start random search *****\")        \n",
    "    for i in range(10):\n",
    "        config_data = random_select()\n",
    "        agent = Agent(config_data)\n",
    "        print(\"*config:\", config_data)\n",
    "        agent.train()\n",
    "        result = []\n",
    "        result.append(config_data)\n",
    "        result.append(agent.moving_avg_list[len(agent.moving_avg_list)-1])\n",
    "        result.append(np.mean(agent.reward_list))\n",
    "        results.append(result)\n",
    "        print(\"*result:\", i, agent.moving_avg_list[len(agent.moving_avg_list)-1], np.mean(agent.reward_list))\n",
    "        print(\"-\"*100)\n",
    "    print(\"***** end random search *****\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_list = []\n",
    "for i in range(0, 100):\n",
    "    avg_list.append([results[i][2], i])\n",
    "avg_list.sort(reverse=True)    \n",
    "avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[27])\n",
    "print(results[31])\n",
    "print(results[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_300 = 0\n",
    "for i in range(0, 100):\n",
    "    if(results[i][2] >= 300):\n",
    "        count_300 = count_300+1\n",
    "print(\"count_400:\", count_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
