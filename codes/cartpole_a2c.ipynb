{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*****end a2c learing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.002\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model_critic()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mAgent.build_model_actor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     51\u001b[0m     input_states \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_states\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     input_action_matrixs \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_action_matrixs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     input_advantages \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_size), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_advantages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m (input_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL2\\lib\\site-packages\\keras\\engine\\input_layer.py:218\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m         batch_input_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m         batch_input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.node_num = 12\n",
    "        self.learning_rate = 0.002\n",
    "        self.epochs_cnt = 1\n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    "        \n",
    "        self.discount_rate = 0.95\n",
    "        self.penalty = -100\n",
    "\n",
    "        self.episode_num = 500\n",
    "        \n",
    "        self.moving_avg_size = 20\n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE = np.zeros((1,self.action_size)), np.zeros((1,self.value_size))\n",
    " \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages = in_datas[0], in_datas[1], in_datas[2]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)  # Forward pass\n",
    "                action_probs = K.max(action_matrixs*y_pred, axis=-1)\n",
    "                loss = -K.log(action_probs)*advantages\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(self.value_size), name='input_advantages')\n",
    "        \n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(self.state_size), name='input_states')\n",
    "        \n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate),\n",
    "                      loss='mean_squared_error'\n",
    "                     )\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        reward_list=[]\n",
    "        count_list = []\n",
    "        moving_avg_list = []\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            self.env.max_episode_steps = 500\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "                \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "            if(episode % 10 == 0):\n",
    "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
    "\n",
    "        self.save_model()\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(state, [1,self.state_size]) #현재상태\n",
    "            action_matrix_t = np.reshape(action_matrix, [1,self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0])[0]\n",
    "            \n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "    \n",
    "            self.train_mini_batch(state, state_next, reward, action_matrix, action_prob, done, count)\n",
    "    \n",
    "            state = state_next\n",
    "                \n",
    "            reward_tot += reward\n",
    "            \n",
    "        return count, reward_tot\n",
    "    \n",
    "    def train_mini_batch(self, state, state_next, reward, action_matrix, action_prob, done, count):\n",
    "        \n",
    "        state_t = np.reshape(state, [1, self.state_size])\n",
    "        state_next_t = np.reshape(state_next, [1, self.state_size])\n",
    "        reward_t = np.reshape(reward, [1, self.value_size])\n",
    "        action_matrix_t = np.reshape(action_matrix, [1, self.action_size])\n",
    "        action_prob_t = np.reshape(action_prob, [1, self.action_size])\n",
    "        \n",
    "        advantage_t = np.zeros((1, self.value_size))\n",
    "        target_t = np.zeros((1, self.value_size))        \n",
    "        \n",
    "        value_t = self.model_critic.predict(state_t)\n",
    "        value_next_t = self.model_critic.predict(state_next_t)\n",
    "        \n",
    "        if(count< 500 and done):\n",
    "            advantage_t =  reward_t - value_t\n",
    "            target_t = reward_t\n",
    "        else:\n",
    "            advantage_t = reward_t + self.discount_rate*value_next_t - value_t\n",
    "            target_t = reward_t + self.discount_rate * value_next_t\n",
    "\n",
    "        self.model_actor.fit(x=[state_t, action_matrix_t, advantage_t], y=[action_prob_t], epochs=self.epochs_cnt, verbose=0)\n",
    "        self.model_critic.fit(x=state_t, y=target_t, epochs=self.epochs_cnt, verbose=0)  \n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model_actor.save(\"./model/a2c\")\n",
    "        print(\"*****end a2c learing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(agent.reward_list, label='rewards')\n",
    "plt.plot(agent.moving_avg_list, linewidth=4, label='moving average')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('A2C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
