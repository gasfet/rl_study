{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | discou... | epochs... | epochs... | layer_... | layer_... | learni... | learni... | loss_c... | mini_b... | node_n... | node_n... |  penalty  | smooth... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m37.41    \u001b[0m | \u001b[0m0.9375   \u001b[0m | \u001b[0m5.161    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1.302    \u001b[0m | \u001b[0m1.147    \u001b[0m | \u001b[0m0.0001831\u001b[0m | \u001b[0m0.0002676\u001b[0m | \u001b[0m0.1691   \u001b[0m | \u001b[0m34.15    \u001b[0m | \u001b[0m74.5     \u001b[0m | \u001b[0m60.63    \u001b[0m | \u001b[0m-164.2   \u001b[0m | \u001b[0m0.9184   \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m22.94    \u001b[0m | \u001b[0m0.979    \u001b[0m | \u001b[0m3.082    \u001b[0m | \u001b[0m5.011    \u001b[0m | \u001b[0m1.417    \u001b[0m | \u001b[0m1.559    \u001b[0m | \u001b[0m0.0002263\u001b[0m | \u001b[0m0.0002783\u001b[0m | \u001b[0m0.2601   \u001b[0m | \u001b[0m77.59    \u001b[0m | \u001b[0m48.36    \u001b[0m | \u001b[0m92.31    \u001b[0m | \u001b[0m-70.57   \u001b[0m | \u001b[0m0.9805   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m17.11    \u001b[0m | \u001b[0m0.9077   \u001b[0m | \u001b[0m3.117    \u001b[0m | \u001b[0m3.509    \u001b[0m | \u001b[0m1.878    \u001b[0m | \u001b[0m1.098    \u001b[0m | \u001b[0m0.000479 \u001b[0m | \u001b[0m0.0009621\u001b[0m | \u001b[0m0.2066   \u001b[0m | \u001b[0m56.58    \u001b[0m | \u001b[0m48.6     \u001b[0m | \u001b[0m91.63    \u001b[0m | \u001b[0m-91.03   \u001b[0m | \u001b[0m0.9016   \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m15.81    \u001b[0m | \u001b[0m0.9675   \u001b[0m | \u001b[0m5.967    \u001b[0m | \u001b[0m5.244    \u001b[0m | \u001b[0m1.28     \u001b[0m | \u001b[0m1.789    \u001b[0m | \u001b[0m0.0001929\u001b[0m | \u001b[0m0.0005031\u001b[0m | \u001b[0m0.2817   \u001b[0m | \u001b[0m26.31    \u001b[0m | \u001b[0m45.38    \u001b[0m | \u001b[0m27.08    \u001b[0m | \u001b[0m-490.5   \u001b[0m | \u001b[0m0.9611   \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m54.44    \u001b[0m | \u001b[95m0.919    \u001b[0m | \u001b[95m3.797    \u001b[0m | \u001b[95m4.475    \u001b[0m | \u001b[95m1.053    \u001b[0m | \u001b[95m1.574    \u001b[0m | \u001b[95m0.0002321\u001b[0m | \u001b[95m0.0006304\u001b[0m | \u001b[95m0.24     \u001b[0m | \u001b[95m11.78    \u001b[0m | \u001b[95m60.03    \u001b[0m | \u001b[95m92.55    \u001b[0m | \u001b[95m-297.1   \u001b[0m | \u001b[95m0.9045   \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m131.8    \u001b[0m | \u001b[95m0.9599   \u001b[0m | \u001b[95m4.711    \u001b[0m | \u001b[95m5.644    \u001b[0m | \u001b[95m1.138    \u001b[0m | \u001b[95m1.217    \u001b[0m | \u001b[95m0.0005546\u001b[0m | \u001b[95m0.0006177\u001b[0m | \u001b[95m0.19     \u001b[0m | \u001b[95m31.48    \u001b[0m | \u001b[95m72.58    \u001b[0m | \u001b[95m51.39    \u001b[0m | \u001b[95m-163.3   \u001b[0m | \u001b[95m0.9665   \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m145.7    \u001b[0m | \u001b[95m0.9424   \u001b[0m | \u001b[95m3.826    \u001b[0m | \u001b[95m5.102    \u001b[0m | \u001b[95m1.779    \u001b[0m | \u001b[95m1.849    \u001b[0m | \u001b[95m0.0005856\u001b[0m | \u001b[95m0.0007415\u001b[0m | \u001b[95m0.2949   \u001b[0m | \u001b[95m34.85    \u001b[0m | \u001b[95m61.93    \u001b[0m | \u001b[95m28.71    \u001b[0m | \u001b[95m-420.1   \u001b[0m | \u001b[95m0.9347   \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m509.1    \u001b[0m | \u001b[95m0.9561   \u001b[0m | \u001b[95m5.377    \u001b[0m | \u001b[95m4.037    \u001b[0m | \u001b[95m1.091    \u001b[0m | \u001b[95m1.536    \u001b[0m | \u001b[95m0.000949 \u001b[0m | \u001b[95m0.0004892\u001b[0m | \u001b[95m0.1358   \u001b[0m | \u001b[95m17.45    \u001b[0m | \u001b[95m90.82    \u001b[0m | \u001b[95m122.4    \u001b[0m | \u001b[95m-133.1   \u001b[0m | \u001b[95m0.9446   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m60.71    \u001b[0m | \u001b[0m0.9352   \u001b[0m | \u001b[0m3.343    \u001b[0m | \u001b[0m5.206    \u001b[0m | \u001b[0m1.199    \u001b[0m | \u001b[0m1.062    \u001b[0m | \u001b[0m0.0009595\u001b[0m | \u001b[0m0.0001351\u001b[0m | \u001b[0m0.2367   \u001b[0m | \u001b[0m44.62    \u001b[0m | \u001b[0m119.1    \u001b[0m | \u001b[0m51.09    \u001b[0m | \u001b[0m-47.23   \u001b[0m | \u001b[0m0.9792   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m17.97    \u001b[0m | \u001b[0m0.9682   \u001b[0m | \u001b[0m5.624    \u001b[0m | \u001b[0m4.292    \u001b[0m | \u001b[0m1.77     \u001b[0m | \u001b[0m1.288    \u001b[0m | \u001b[0m0.0002582\u001b[0m | \u001b[0m0.0005829\u001b[0m | \u001b[0m0.2948   \u001b[0m | \u001b[0m36.58    \u001b[0m | \u001b[0m83.53    \u001b[0m | \u001b[0m23.39    \u001b[0m | \u001b[0m-362.3   \u001b[0m | \u001b[0m0.9589   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m161.6    \u001b[0m | \u001b[0m0.9226   \u001b[0m | \u001b[0m3.466    \u001b[0m | \u001b[0m5.953    \u001b[0m | \u001b[0m1.183    \u001b[0m | \u001b[0m1.364    \u001b[0m | \u001b[0m0.0004184\u001b[0m | \u001b[0m0.0002818\u001b[0m | \u001b[0m0.2086   \u001b[0m | \u001b[0m10.69    \u001b[0m | \u001b[0m119.6    \u001b[0m | \u001b[0m18.01    \u001b[0m | \u001b[0m-368.9   \u001b[0m | \u001b[0m0.9539   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m231.8    \u001b[0m | \u001b[0m0.9705   \u001b[0m | \u001b[0m5.528    \u001b[0m | \u001b[0m4.525    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.638    \u001b[0m | \u001b[0m0.0009798\u001b[0m | \u001b[0m0.0003294\u001b[0m | \u001b[0m0.1266   \u001b[0m | \u001b[0m11.18    \u001b[0m | \u001b[0m96.64    \u001b[0m | \u001b[0m128.0    \u001b[0m | \u001b[0m-130.5   \u001b[0m | \u001b[0m0.9512   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m72.64    \u001b[0m | \u001b[0m0.9812   \u001b[0m | \u001b[0m5.096    \u001b[0m | \u001b[0m5.418    \u001b[0m | \u001b[0m1.202    \u001b[0m | \u001b[0m1.007    \u001b[0m | \u001b[0m0.0003334\u001b[0m | \u001b[0m0.0009946\u001b[0m | \u001b[0m0.2786   \u001b[0m | \u001b[0m30.67    \u001b[0m | \u001b[0m78.82    \u001b[0m | \u001b[0m48.89    \u001b[0m | \u001b[0m-161.2   \u001b[0m | \u001b[0m0.9501   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m73.33    \u001b[0m | \u001b[0m0.9821   \u001b[0m | \u001b[0m5.987    \u001b[0m | \u001b[0m5.647    \u001b[0m | \u001b[0m1.1      \u001b[0m | \u001b[0m1.971    \u001b[0m | \u001b[0m0.0009659\u001b[0m | \u001b[0m0.0007482\u001b[0m | \u001b[0m0.2833   \u001b[0m | \u001b[0m18.29    \u001b[0m | \u001b[0m85.98    \u001b[0m | \u001b[0m121.8    \u001b[0m | \u001b[0m-132.2   \u001b[0m | \u001b[0m0.9888   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m47.57    \u001b[0m | \u001b[0m0.9873   \u001b[0m | \u001b[0m4.764    \u001b[0m | \u001b[0m4.072    \u001b[0m | \u001b[0m1.858    \u001b[0m | \u001b[0m1.077    \u001b[0m | \u001b[0m0.0004358\u001b[0m | \u001b[0m0.0004647\u001b[0m | \u001b[0m0.2783   \u001b[0m | \u001b[0m39.85    \u001b[0m | \u001b[0m55.34    \u001b[0m | \u001b[0m21.24    \u001b[0m | \u001b[0m-484.9   \u001b[0m | \u001b[0m0.9184   \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m29.05    \u001b[0m | \u001b[0m0.9692   \u001b[0m | \u001b[0m3.7      \u001b[0m | \u001b[0m5.935    \u001b[0m | \u001b[0m1.737    \u001b[0m | \u001b[0m1.245    \u001b[0m | \u001b[0m0.0001858\u001b[0m | \u001b[0m0.0007444\u001b[0m | \u001b[0m0.1418   \u001b[0m | \u001b[0m72.77    \u001b[0m | \u001b[0m38.84    \u001b[0m | \u001b[0m66.25    \u001b[0m | \u001b[0m-443.6   \u001b[0m | \u001b[0m0.954    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m27.72    \u001b[0m | \u001b[0m0.9384   \u001b[0m | \u001b[0m4.063    \u001b[0m | \u001b[0m5.433    \u001b[0m | \u001b[0m1.973    \u001b[0m | \u001b[0m1.702    \u001b[0m | \u001b[0m0.0002531\u001b[0m | \u001b[0m0.0004836\u001b[0m | \u001b[0m0.2822   \u001b[0m | \u001b[0m20.95    \u001b[0m | \u001b[0m14.45    \u001b[0m | \u001b[0m56.22    \u001b[0m | \u001b[0m-360.9   \u001b[0m | \u001b[0m0.9325   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m27.06    \u001b[0m | \u001b[0m0.9874   \u001b[0m | \u001b[0m4.124    \u001b[0m | \u001b[0m5.531    \u001b[0m | \u001b[0m1.317    \u001b[0m | \u001b[0m1.099    \u001b[0m | \u001b[0m0.0006489\u001b[0m | \u001b[0m0.0001439\u001b[0m | \u001b[0m0.1877   \u001b[0m | \u001b[0m64.63    \u001b[0m | \u001b[0m66.42    \u001b[0m | \u001b[0m46.25    \u001b[0m | \u001b[0m-324.1   \u001b[0m | \u001b[0m0.9804   \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m156.7    \u001b[0m | \u001b[0m0.9468   \u001b[0m | \u001b[0m5.848    \u001b[0m | \u001b[0m4.31     \u001b[0m | \u001b[0m1.338    \u001b[0m | \u001b[0m1.942    \u001b[0m | \u001b[0m0.0002304\u001b[0m | \u001b[0m0.0001433\u001b[0m | \u001b[0m0.1504   \u001b[0m | \u001b[0m17.42    \u001b[0m | \u001b[0m119.2    \u001b[0m | \u001b[0m72.16    \u001b[0m | \u001b[0m-343.8   \u001b[0m | \u001b[0m0.9182   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m19.81    \u001b[0m | \u001b[0m0.9827   \u001b[0m | \u001b[0m5.733    \u001b[0m | \u001b[0m4.683    \u001b[0m | \u001b[0m1.874    \u001b[0m | \u001b[0m1.492    \u001b[0m | \u001b[0m0.0002364\u001b[0m | \u001b[0m0.0008774\u001b[0m | \u001b[0m0.1281   \u001b[0m | \u001b[0m73.26    \u001b[0m | \u001b[0m27.1     \u001b[0m | \u001b[0m24.63    \u001b[0m | \u001b[0m-295.4   \u001b[0m | \u001b[0m0.9433   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m98.94    \u001b[0m | \u001b[0m0.9408   \u001b[0m | \u001b[0m5.98     \u001b[0m | \u001b[0m3.106    \u001b[0m | \u001b[0m1.338    \u001b[0m | \u001b[0m1.537    \u001b[0m | \u001b[0m0.0005013\u001b[0m | \u001b[0m0.0001813\u001b[0m | \u001b[0m0.2879   \u001b[0m | \u001b[0m67.19    \u001b[0m | \u001b[0m68.89    \u001b[0m | \u001b[0m95.38    \u001b[0m | \u001b[0m-89.85   \u001b[0m | \u001b[0m0.9136   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m56.22    \u001b[0m | \u001b[0m0.9408   \u001b[0m | \u001b[0m4.317    \u001b[0m | \u001b[0m4.081    \u001b[0m | \u001b[0m1.172    \u001b[0m | \u001b[0m1.664    \u001b[0m | \u001b[0m0.0003451\u001b[0m | \u001b[0m0.0009641\u001b[0m | \u001b[0m0.1131   \u001b[0m | \u001b[0m57.73    \u001b[0m | \u001b[0m115.6    \u001b[0m | \u001b[0m47.91    \u001b[0m | \u001b[0m-106.4   \u001b[0m | \u001b[0m0.9336   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m19.16    \u001b[0m | \u001b[0m0.9044   \u001b[0m | \u001b[0m3.296    \u001b[0m | \u001b[0m4.876    \u001b[0m | \u001b[0m1.686    \u001b[0m | \u001b[0m1.858    \u001b[0m | \u001b[0m0.0002184\u001b[0m | \u001b[0m0.0006983\u001b[0m | \u001b[0m0.2079   \u001b[0m | \u001b[0m47.25    \u001b[0m | \u001b[0m91.28    \u001b[0m | \u001b[0m68.71    \u001b[0m | \u001b[0m-215.7   \u001b[0m | \u001b[0m0.9858   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m18.91    \u001b[0m | \u001b[0m0.9605   \u001b[0m | \u001b[0m3.133    \u001b[0m | \u001b[0m4.512    \u001b[0m | \u001b[0m1.243    \u001b[0m | \u001b[0m1.388    \u001b[0m | \u001b[0m0.0001527\u001b[0m | \u001b[0m0.0009078\u001b[0m | \u001b[0m0.1728   \u001b[0m | \u001b[0m76.53    \u001b[0m | \u001b[0m87.38    \u001b[0m | \u001b[0m82.61    \u001b[0m | \u001b[0m-336.0   \u001b[0m | \u001b[0m0.9774   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m387.3    \u001b[0m | \u001b[0m0.9475   \u001b[0m | \u001b[0m5.208    \u001b[0m | \u001b[0m5.257    \u001b[0m | \u001b[0m1.277    \u001b[0m | \u001b[0m1.617    \u001b[0m | \u001b[0m0.0006155\u001b[0m | \u001b[0m0.0007829\u001b[0m | \u001b[0m0.1399   \u001b[0m | \u001b[0m27.28    \u001b[0m | \u001b[0m97.4     \u001b[0m | \u001b[0m106.6    \u001b[0m | \u001b[0m-411.5   \u001b[0m | \u001b[0m0.9122   \u001b[0m |\n",
      "=====================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, config_data):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.layer_num_actor = int(round(config_data['layer_num_actor'],0))\n",
    "        self.node_num_actor = int(round(config_data['node_num_actor'],0))\n",
    "        self.epochs_actor = int(round(config_data['epochs_actor'],0))\n",
    "        self.layer_num_critic = int(round(config_data['layer_num_critic'],0))\n",
    "        self.node_num_critic = int(round(config_data['node_num_critic'],0))\n",
    "        self.epochs_critic = int(round(config_data['epochs_critic'],0))\n",
    "        \n",
    "        self.learning_rate_actor = config_data['learning_rate_actor']\n",
    "        self.learning_rate_critic = config_data['learning_rate_critic']\n",
    "        self.discount_rate = config_data['discount_rate']\n",
    "        self.smooth_rate = config_data['smooth_rate']\n",
    "        self.penalty = int(round(config_data['penalty'],0))\n",
    "        self.mini_batch_step_size = int(round(config_data['mini_batch_step_size'],0))\n",
    "        self.loss_clipping = config_data['loss_clipping']\n",
    "\n",
    "        self.episode_num = 100\n",
    "        self.moving_avg_size = 20\n",
    "        \n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    " \n",
    "        self.states, self.states_next, self.action_matrixs, self.dones, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE = np.zeros((1,1,self.action_size)), np.zeros((1,1,self.value_size))\n",
    "    \n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages, loss_clipping = in_datas[0], in_datas[1], in_datas[2], in_datas[3]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)    \n",
    "                r = new_policy/(old_policy)\n",
    "                \n",
    "                LOSS_CLIPPING = K.mean(loss_clipping)\n",
    "                \n",
    "                loss = -K.minimum(r*advantages, K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)*advantages)\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(1,self.value_size), name='input_advantages')\n",
    "        input_loss_clipping = Input(shape=(1,self.value_size), name='input_loss_clipping')        \n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_actor+1):            \n",
    "            x = Dense(self.node_num_actor, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_actor))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        \n",
    "        x = (input_states)\n",
    "        for i in range(1,self.layer_num_critic+1):\n",
    "            x = Dense(self.node_num_critic, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate_critic),\n",
    "#                       loss='mean_squared_error'\n",
    "                      loss = \"binary_crossentropy\"\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "            \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "\n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states, self.states_next, self.action_matrixs, self.done, self.action_probs, self.rewards = [],[],[],[],[],[]\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        advantage = np.zeros(self.value_size)\n",
    "        target = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(self.normalize(state),[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "            \n",
    "            state_next_t = np.reshape(self.normalize(state_next),[1, 1, self.state_size])\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "        \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.states_next.append(np.reshape(state_next_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.dones.append(np.reshape(0 if done else 1, [1,self.value_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(np.reshape(reward, [1,self.value_size]))\n",
    "            \n",
    "            if(count % self.mini_batch_step_size == 0):\n",
    "                self.train_mini_batch()\n",
    "                self.clear_memory()\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "            \n",
    "        return count, reward_tot\n",
    "    \n",
    "    def make_gae(self, values, values_next, rewards, dones):\n",
    "        delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "        advantages = np.zeros(np.array(values).shape)\n",
    "        targets = np.zeros(np.array(values).shape)\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            delta_adv = rewards[t] + self.discount_rate * values_next[t] * dones[t] - values[t]\n",
    "            delta_tar = rewards[t] + self.discount_rate * values_next[t] * dones[t]\n",
    "            adv = delta_adv + self.smooth_rate *  self.discount_rate * dones[t] * adv\n",
    "            target = delta_tar + self.smooth_rate * self.discount_rate * dones[t] * target\n",
    "            advantages[t] = adv\n",
    "            targets[t] = target\n",
    "        return advantages, targets\n",
    "\n",
    "    def normalize(self, x):\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0: \n",
    "            return x\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        \n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        states_t = np.array(self.states)\n",
    "        states_next_t = np.array(self.states_next)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        loss_clipping = [self.loss_clipping for j in range(len(self.states))]\n",
    "        loss_clipping_t = np.reshape(loss_clipping, [len(self.states),1,1])\n",
    "        \n",
    "        values = self.model_critic.predict(states_t)\n",
    "        values_next = self.model_critic.predict(states_next_t)\n",
    "        \n",
    "        advantages, targets = self.make_gae(values, values_next, self.rewards, self.dones)\n",
    "        advantages_t = np.array(advantages)\n",
    "        targets_t = np.array(targets)\n",
    "        \n",
    "        self.model_actor.fit([states_t, action_matrixs_t, advantages_t, loss_clipping_t], [action_probs_t], \n",
    "                             epochs=self.epochs_actor, verbose=0)\n",
    "        self.model_critic.fit(states_t, targets_t, \n",
    "                              epochs=self.epochs_critic, verbose=0)       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def black_box_function(layer_num_actor, node_num_actor, epochs_actor, \n",
    "                           layer_num_critic, node_num_critic, epochs_critic,\n",
    "                           learning_rate_actor, learning_rate_critic,\n",
    "                           discount_rate, smooth_rate, \n",
    "                           penalty, mini_batch_step_size, loss_clipping\n",
    "                          ):\n",
    "        config_data = {\n",
    "            'layer_num_actor':layer_num_actor,\n",
    "            'node_num_actor':node_num_actor,\n",
    "            'epochs_actor':epochs_actor,\n",
    "            'layer_num_critic':layer_num_critic,\n",
    "            'node_num_critic':node_num_critic,\n",
    "            'epochs_critic':epochs_critic,\n",
    "            \n",
    "            'learning_rate_actor' :learning_rate_actor,\n",
    "            'learning_rate_critic':learning_rate_critic,\n",
    "            'discount_rate'       :discount_rate,\n",
    "            'smooth_rate'       :smooth_rate,\n",
    "            'penalty'             :penalty,\n",
    "            'mini_batch_step_size':mini_batch_step_size,\n",
    "            'loss_clipping'       :loss_clipping\n",
    "        }\n",
    "        agent = Agent(config_data)\n",
    "        agent.train()\n",
    "        return np.mean(agent.reward_list)\n",
    "        \n",
    "    pbounds = {\n",
    "                'layer_num_actor':(1,2),\n",
    "                'node_num_actor':(12,128),\n",
    "                'epochs_actor':(3,6),\n",
    "                'layer_num_critic':(1,2),\n",
    "                'node_num_critic':(12,128),\n",
    "                'epochs_critic':(3,6),\n",
    "\n",
    "                'learning_rate_actor' :(0.0001,0.001),\n",
    "                'learning_rate_critic':(0.0001,0.001),\n",
    "                'discount_rate'       :(0.9,0.99),\n",
    "                'smooth_rate'       :(0.9,0.99),\n",
    "                'penalty'             :(-500,-10),\n",
    "                'mini_batch_step_size':(4,80),\n",
    "                'loss_clipping'       :(0.1,0.3)\n",
    "              }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=black_box_function,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=5,\n",
    "        n_iter=20\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[509.1, 7],\n",
       " [387.3, 24],\n",
       " [231.78, 11],\n",
       " [161.59, 10],\n",
       " [156.66, 18],\n",
       " [145.67, 6],\n",
       " [131.78, 5],\n",
       " [98.94, 20],\n",
       " [73.33, 13],\n",
       " [72.64, 12],\n",
       " [60.71, 8],\n",
       " [56.22, 21],\n",
       " [54.44, 4],\n",
       " [47.57, 14],\n",
       " [37.41, 0],\n",
       " [29.05, 15],\n",
       " [27.72, 16],\n",
       " [27.06, 17],\n",
       " [22.94, 1],\n",
       " [19.81, 19],\n",
       " [19.16, 22],\n",
       " [18.91, 23],\n",
       " [17.97, 9],\n",
       " [17.11, 2],\n",
       " [15.81, 3]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = []\n",
    "i=0\n",
    "for res in optimizer.res:\n",
    "    target_list.append([res[\"target\"], i])\n",
    "    i=i+1\n",
    "target_list.sort(reverse=True)    \n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*result: {'discount_rate': 0.9408094082997873, 'epochs_actor': 5.980291597720334, 'epochs_critic': 3.1063285250605923, 'layer_num_actor': 1.3375245480618607, 'layer_num_critic': 1.5374126640170411, 'learning_rate_actor': 0.0005012795002669534, 'learning_rate_critic': 0.00018131536632788256, 'loss_clipping': 0.2878898770514272, 'mini_batch_step_size': 67.18528480210719, 'node_num_actor': 68.88767531848582, 'node_num_critic': 95.37597827905797, 'penalty': -89.85108034185885, 'smooth_rate': 0.9136090621999824}\n"
     ]
    }
   ],
   "source": [
    "print(\"*result:\" , optimizer.res[20]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_100: 7\n"
     ]
    }
   ],
   "source": [
    "count_100 = 0\n",
    "for res in optimizer.res:\n",
    "    if(res[\"target\"] >= 100):\n",
    "        count_100 = count_100+1\n",
    "print(\"count_100:\", count_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
